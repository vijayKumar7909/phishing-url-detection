{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:43:59.954194Z","iopub.execute_input":"2025-06-17T05:43:59.955081Z","iopub.status.idle":"2025-06-17T05:44:00.246745Z","shell.execute_reply.started":"2025-06-17T05:43:59.955052Z","shell.execute_reply":"2025-06-17T05:44:00.246062Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/phishing-website-detector/phishing.txt\n/kaggle/input/phishing-website-detector/phishing.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -U numpy==1.25.2 scikit-learn==1.4.2 imbalanced-learn==0.12.0\n!pip install tldextract\n!pip install scikeras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:00.248313Z","iopub.execute_input":"2025-06-17T05:44:00.248654Z","iopub.status.idle":"2025-06-17T05:44:18.964666Z","shell.execute_reply.started":"2025-06-17T05:44:00.248634Z","shell.execute_reply":"2025-06-17T05:44:18.963914Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.25.2\n  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting scikit-learn==1.4.2\n  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.12.0\n  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\nDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scikit-learn, imbalanced-learn\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: imbalanced-learn\n    Found existing installation: imbalanced-learn 0.13.0\n    Uninstalling imbalanced-learn-0.13.0:\n      Successfully uninstalled imbalanced-learn-0.13.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.25.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.12.0 numpy-1.25.2 scikit-learn-1.4.2\nCollecting tldextract\n  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\nRequirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\nCollecting requests-file>=1.4 (from tldextract)\n  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.4.26)\nDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\nInstalling collected packages: requests-file, tldextract\nSuccessfully installed requests-file-2.1.0 tldextract-5.3.0\nCollecting scikeras\n  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\nRequirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.4.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.13.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (25.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.13.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\nDownloading scikeras-0.13.0-py3-none-any.whl (26 kB)\nInstalling collected packages: scikeras\nSuccessfully installed scikeras-0.13.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Loading dataset\nimport pandas as pd\n\n# Load data\ncsv_data = pd.read_csv('/kaggle/input/phishing-website-detector/phishing.csv')\nprint(\"Dataset Loaded. Shape:\", csv_data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:18.965612Z","iopub.execute_input":"2025-06-17T05:44:18.965894Z","iopub.status.idle":"2025-06-17T05:44:19.022231Z","shell.execute_reply.started":"2025-06-17T05:44:18.965843Z","shell.execute_reply":"2025-06-17T05:44:19.021628Z"}},"outputs":[{"name":"stdout","text":"Dataset Loaded. Shape: (11054, 32)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Dataset operations\nimport pandas as pd\nimport tldextract\nimport urllib.parse\nfrom math import log\n\n# Feature extraction function\ndef calculate_entropy(s):\n    freq = {}\n    for c in s:\n        freq[c] = freq.get(c, 0) + 1\n    return -sum((f/len(s)) * log(f/len(s), 2) for f in freq.values() if f) if len(s) > 0 else 0\n\ndef extract_features_for_training(row, url_column='URL'):\n    try:\n        url = row[url_column]\n        parsed_url = urllib.parse.urlparse(url if url.startswith(('http://', 'https://')) else f'http://{url}')\n        extracted = tldextract.extract(url)\n        domain_len = len(extracted.domain)\n        subdomain_count = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n        path = parsed_url.path.lower()\n        domain = extracted.domain.lower()\n        suspicious_keywords = ['login', 'secure', 'account', 'verify', 'update', 'auth']\n        tld = extracted.suffix.lower()\n        is_local_ip = parsed_url.hostname and parsed_url.hostname.startswith('192.168.')\n        entropy = calculate_entropy(domain)\n        https = 1 if parsed_url.scheme == 'https' else -1\n        features = {\n            'AnchorURL': 1 if any(kw in path or kw in domain for kw in suspicious_keywords) else -1,\n            'WebsiteTraffic': 1 if domain_len < 7 and tld in ['com', 'org', 'edu', 'gov'] else -1,\n            'AgeofDomain': 1 if domain_len < 9 and tld in ['com', 'org', 'gov'] else -1,\n            'PageRank': 1 if domain_len < 7 and tld in ['com', 'org', 'edu', 'gov'] else -1,\n            'PrefixSuffix-': 1 if '-' in extracted.domain else -1,\n            'UsingIP': 1 if is_local_ip else -2 if parsed_url.hostname and parsed_url.hostname.replace('.', '').isdigit() else -1,\n            'SubDomains': -4 if subdomain_count >= 2 else -2 if subdomain_count == 1 else 0,\n            'Entropy': entropy,\n            'HTTPS': https\n        }\n        return pd.Series(features)\n    except Exception as e:\n        print(f\"Error processing URL {url}: {e}\")\n        return pd.Series({'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, \n                          'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 0, 'HTTPS': -1})\n\n# Check for URL column\npossible_url_columns = ['URL', 'url', 'website', 'domain']\nurl_column = next((col for col in possible_url_columns if col in csv_data.columns), None)\nif url_column is None:\n    print(\"Warning: No URL column found. Using pre-existing features if available.\")\n    selected_features = [col for col in csv_data.columns if col != 'class' and csv_data[col].std() > 0]\nelse:\n    csv_data['URL'] = csv_data[url_column]\n    selected_features = ['AnchorURL', 'WebsiteTraffic', 'AgeofDomain', 'PageRank', \n                        'PrefixSuffix-', 'UsingIP', 'SubDomains', 'Entropy', 'HTTPS']\n\n# Verify no duplicate features\nif len(selected_features) != len(set(selected_features)):\n    raise ValueError(f\"Duplicate features detected in selected_features: {selected_features}\")\n\n# Check for class column and valid labels\nif 'class' not in csv_data.columns:\n    raise ValueError(\"Dataset must contain a 'class' column\")\nif not set(csv_data['class']).issubset({-1, 1}):\n    raise ValueError(\"Class labels must be -1 or 1\")\n\n# Extract features if URL column exists\nif url_column:\n    existing_columns = csv_data.columns\n    feature_df = csv_data.apply(extract_features_for_training, axis=1)\n    feature_df.columns = [f\"new_{col}\" if col in existing_columns else col for col in feature_df.columns]\n    selected_features = [f\"new_{col}\" if col in existing_columns else col for col in selected_features]\n    csv_data = pd.concat([csv_data, feature_df], axis=1)\n    csv_data = csv_data.loc[:, ~csv_data.columns.duplicated()]\nelse:\n    csv_data = csv_data[selected_features + ['class']]\n\n# Remove constant features\nselected_features = [col for col in selected_features if csv_data[col].std() > 0]\nif not selected_features:\n    raise ValueError(\"No features with variance found. Check feature extraction or dataset.\")\n\nprint(\"Data Shape:\", csv_data.shape)\nprint(\"\\nClass Distribution:\\n\", csv_data['class'].value_counts())\nprint(\"\\nFeature Stats:\\n\", csv_data[selected_features].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:19.022960Z","iopub.execute_input":"2025-06-17T05:44:19.023136Z","iopub.status.idle":"2025-06-17T05:44:19.246872Z","shell.execute_reply.started":"2025-06-17T05:44:19.023121Z","shell.execute_reply":"2025-06-17T05:44:19.246233Z"}},"outputs":[{"name":"stdout","text":"Warning: No URL column found. Using pre-existing features if available.\nData Shape: (11054, 32)\n\nClass Distribution:\n class\n 1    6157\n-1    4897\nName: count, dtype: int64\n\nFeature Stats:\n               Index       UsingIP       LongURL      ShortURL       Symbol@  \\\ncount  11054.000000  11054.000000  11054.000000  11054.000000  11054.000000   \nmean    5526.500000      0.313914     -0.633345      0.738737      0.700561   \nstd     3191.159272      0.949495      0.765973      0.674024      0.713625   \nmin        0.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n25%     2763.250000     -1.000000     -1.000000      1.000000      1.000000   \n50%     5526.500000      1.000000     -1.000000      1.000000      1.000000   \n75%     8289.750000      1.000000     -1.000000      1.000000      1.000000   \nmax    11053.000000      1.000000      1.000000      1.000000      1.000000   \n\n       Redirecting//  PrefixSuffix-    SubDomains         HTTPS  DomainRegLen  \\\ncount   11054.000000   11054.000000  11054.000000  11054.000000  11054.000000   \nmean        0.741632      -0.734938      0.064049      0.251040     -0.336711   \nstd         0.670837       0.678165      0.817492      0.911856      0.941651   \nmin        -1.000000      -1.000000     -1.000000     -1.000000     -1.000000   \n25%         1.000000      -1.000000     -1.000000     -1.000000     -1.000000   \n50%         1.000000      -1.000000      0.000000      1.000000     -1.000000   \n75%         1.000000      -1.000000      1.000000      1.000000      1.000000   \nmax         1.000000       1.000000      1.000000      1.000000      1.000000   \n\n       ...  DisableRightClick  UsingPopupWindow  IframeRedirection  \\\ncount  ...       11054.000000      11054.000000       11054.000000   \nmean   ...           0.913877          0.613353           0.816899   \nstd    ...           0.406009          0.789845           0.576807   \nmin    ...          -1.000000         -1.000000          -1.000000   \n25%    ...           1.000000          1.000000           1.000000   \n50%    ...           1.000000          1.000000           1.000000   \n75%    ...           1.000000          1.000000           1.000000   \nmax    ...           1.000000          1.000000           1.000000   \n\n        AgeofDomain  DNSRecording  WebsiteTraffic      PageRank   GoogleIndex  \\\ncount  11054.000000  11054.000000    11054.000000  11054.000000  11054.000000   \nmean       0.061335      0.377239        0.287407     -0.483626      0.721549   \nstd        0.998162      0.926158        0.827680      0.875314      0.692395   \nmin       -1.000000     -1.000000       -1.000000     -1.000000     -1.000000   \n25%       -1.000000     -1.000000        0.000000     -1.000000      1.000000   \n50%        1.000000      1.000000        1.000000     -1.000000      1.000000   \n75%        1.000000      1.000000        1.000000      1.000000      1.000000   \nmax        1.000000      1.000000        1.000000      1.000000      1.000000   \n\n       LinksPointingToPage   StatsReport  \ncount         11054.000000  11054.000000  \nmean              0.343948      0.719739  \nstd               0.569936      0.694276  \nmin              -1.000000     -1.000000  \n25%               0.000000      1.000000  \n50%               0.000000      1.000000  \n75%               1.000000      1.000000  \nmax               1.000000      1.000000  \n\n[8 rows x 31 columns]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Dataset preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\n# Prepare data\nX = csv_data[selected_features]\ny = csv_data['class'].map({-1: 0, 1: 1})\n\n# Add synthetic data with SMOTE\nsmote = SMOTE(random_state=42)\nX_aug, y_aug = smote.fit_resample(X, y)\naugmented_data = pd.DataFrame(X_aug, columns=selected_features)\naugmented_data['label'] = y_aug\nprint(\"\\nAugmented Data Shape:\", augmented_data.shape)\n\n# Save scaler for later use\nscaler = StandardScaler()\nX_aug_scaled = scaler.fit_transform(X_aug)\njoblib.dump(scaler, '/kaggle/working/scaler.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:19.248486Z","iopub.execute_input":"2025-06-17T05:44:19.248714Z","iopub.status.idle":"2025-06-17T05:44:20.570414Z","shell.execute_reply.started":"2025-06-17T05:44:19.248695Z","shell.execute_reply":"2025-06-17T05:44:20.569587Z"}},"outputs":[{"name":"stdout","text":"\nAugmented Data Shape: (12314, 32)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/scaler.pkl']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Splitting dataset into training and testing\nfrom sklearn.model_selection import train_test_split\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, test_size=0.2, random_state=42)\nprint(\"Training Shape:\", X_train.shape, y_train.shape)\nprint(\"Testing Shape:\", X_test.shape, y_test.shape)\n\n# Scale features\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:20.571281Z","iopub.execute_input":"2025-06-17T05:44:20.571706Z","iopub.status.idle":"2025-06-17T05:44:20.591377Z","shell.execute_reply.started":"2025-06-17T05:44:20.571685Z","shell.execute_reply":"2025-06-17T05:44:20.590765Z"}},"outputs":[{"name":"stdout","text":"Training Shape: (9851, 31) (9851,)\nTesting Shape: (2463, 31) (2463,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Training Random Forest model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for Random Forest\nparam_grid_rf = {\n    'estimator__n_estimators': [100, 200],\n    'estimator__max_depth': [10, 20, None],\n    'estimator__min_samples_split': [2, 5]\n}\ngrid_search_rf = GridSearchCV(\n    CalibratedClassifierCV(RandomForestClassifier(class_weight='balanced', random_state=42), method='sigmoid', cv=5),\n    param_grid_rf, scoring='f1', cv=5, n_jobs=-1\n)\ngrid_search_rf.fit(X_train_scaled, y_train)\nbest_rf_params = grid_search_rf.best_params_\nprint(f\"Best Random Forest Params: {best_rf_params}\")\n\n# Train Random Forest with best parameters\nrf_model = CalibratedClassifierCV(\n    RandomForestClassifier(**{k.split('__')[1]: v for k, v in best_rf_params.items()},\n                          class_weight='balanced', random_state=42),\n    method='sigmoid', cv=5\n)\nrf_model.fit(X_train_scaled, y_train)\njoblib.dump(rf_model, '/kaggle/working/calibrated_random_forest_model.pkl')\nprint(\"Random Forest model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:44:20.592050Z","iopub.execute_input":"2025-06-17T05:44:20.592344Z","iopub.status.idle":"2025-06-17T05:45:54.988199Z","shell.execute_reply.started":"2025-06-17T05:44:20.592317Z","shell.execute_reply":"2025-06-17T05:45:54.987459Z"}},"outputs":[{"name":"stdout","text":"Best Random Forest Params: {'estimator__max_depth': 20, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 100}\nRandom Forest model trained and saved.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Testing Random Forest model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate Random Forest\ny_pred_rf = rf_model.predict(X_test_scaled)\ny_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\nrf_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_rf),\n    'Precision': precision_score(y_test, y_pred_rf, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_rf, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_rf, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_rf)\n}\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nprint(\"\\nRandom Forest Metrics:\")\nprint(f\"Accuracy: {rf_results['Accuracy']:.4f}\")\nprint(f\"Precision: {rf_results['Precision']:.4f}\")\nprint(f\"Recall: {rf_results['Recall']:.4f}\")\nprint(f\"F1-Score: {rf_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {rf_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_rf}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('Random Forest Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/random_forest_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, y_pred_proba_rf)\noptimal_idx_rf = np.argmax(precision_rf * recall_rf)\noptimal_threshold_rf = thresholds_rf[optimal_idx_rf]\nprint(f\"Random Forest Optimal Threshold: {optimal_threshold_rf:.2f}\")\n\n# Feature importance\nfeature_importance_rf = pd.Series(rf_model.calibrated_classifiers_[0].estimator.feature_importances_, index=selected_features)\nprint(\"\\nRandom Forest Feature Importances:\\n\", feature_importance_rf.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:45:54.989056Z","iopub.execute_input":"2025-06-17T05:45:54.989310Z","iopub.status.idle":"2025-06-17T05:45:55.720398Z","shell.execute_reply.started":"2025-06-17T05:45:54.989280Z","shell.execute_reply":"2025-06-17T05:45:55.719666Z"}},"outputs":[{"name":"stdout","text":"\nRandom Forest Metrics:\nAccuracy: 0.9683\nPrecision: 0.9602\nRecall: 0.9761\nF1-Score: 0.9681\nROC-AUC: 0.9968\nConfusion Matrix:\n[[1203   49]\n [  29 1182]]\nRandom Forest Optimal Threshold: 0.64\n\nRandom Forest Feature Importances:\n HTTPS                  0.316367\nAnchorURL              0.196381\nWebsiteTraffic         0.083207\nSubDomains             0.075179\nIndex                  0.060662\nPrefixSuffix-          0.039338\nLinksInScriptTags      0.033984\nRequestURL             0.031653\nDomainRegLen           0.019056\nServerFormHandler      0.017614\nLinksPointingToPage    0.015102\nAgeofDomain            0.012805\nUsingIP                0.011426\nGoogleIndex            0.010206\nDNSRecording           0.009569\nPageRank               0.009270\nLongURL                0.006886\nUsingPopupWindow       0.005128\nShortURL               0.004994\nHTTPSDomainURL         0.004890\nInfoEmail              0.004554\nStatsReport            0.003883\nRedirecting//          0.003868\nSymbol@                0.003804\nFavicon                0.003786\nAbnormalURL            0.003694\nWebsiteForwarding      0.003556\nStatusBarCust          0.003142\nNonStdPort             0.002713\nIframeRedirection      0.002207\nDisableRightClick      0.001078\ndtype: float64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Training XGBoost model\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Train XGBoost\nxgb_model = CalibratedClassifierCV(\n    XGBClassifier(learning_rate=0.1, max_depth=6, n_estimators=100,\n                  scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]), random_state=42),\n    method='sigmoid', cv=5\n)\nxgb_model.fit(X_train_scaled, y_train)\njoblib.dump(xgb_model, '/kaggle/working/calibrated_xgboost_model.pkl')\nprint(\"XGBoost model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:45:55.721231Z","iopub.execute_input":"2025-06-17T05:45:55.721784Z","iopub.status.idle":"2025-06-17T05:45:57.208085Z","shell.execute_reply.started":"2025-06-17T05:45:55.721731Z","shell.execute_reply":"2025-06-17T05:45:57.204901Z"}},"outputs":[{"name":"stdout","text":"XGBoost model trained and saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Testing XGBoost model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate XGBoost\ny_pred_xgb = xgb_model.predict(X_test_scaled)\ny_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\nxgb_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n    'Precision': precision_score(y_test, y_pred_xgb, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_xgb, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_xgb, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_xgb)\n}\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\nprint(\"\\nXGBoost Metrics:\")\nprint(f\"Accuracy: {xgb_results['Accuracy']:.4f}\")\nprint(f\"Precision: {xgb_results['Precision']:.4f}\")\nprint(f\"Recall: {xgb_results['Recall']:.4f}\")\nprint(f\"F1-Score: {xgb_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {xgb_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_xgb}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('XGBoost Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/xgboost_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_xgb, recall_xgb, thresholds_xgb = precision_recall_curve(y_test, y_pred_proba_xgb)\noptimal_idx_xgb = np.argmax(precision_xgb * recall_xgb)\noptimal_threshold_xgb = thresholds_xgb[optimal_idx_xgb]\nprint(f\"XGBoost Optimal Threshold: {optimal_threshold_xgb:.2f}\")\n\n# Feature importance\nfeature_importance_xgb = pd.Series(xgb_model.calibrated_classifiers_[0].estimator.feature_importances_, index=selected_features)\nprint(\"\\nXGBoost Feature Importances:\\n\", feature_importance_xgb.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:45:57.208799Z","iopub.execute_input":"2025-06-17T05:45:57.209298Z","iopub.status.idle":"2025-06-17T05:45:57.416967Z","shell.execute_reply.started":"2025-06-17T05:45:57.209276Z","shell.execute_reply":"2025-06-17T05:45:57.416363Z"}},"outputs":[{"name":"stdout","text":"\nXGBoost Metrics:\nAccuracy: 0.9659\nPrecision: 0.9600\nRecall: 0.9711\nF1-Score: 0.9655\nROC-AUC: 0.9965\nConfusion Matrix:\n[[1203   49]\n [  35 1176]]\nXGBoost Optimal Threshold: 0.77\n\nXGBoost Feature Importances:\n HTTPS                  0.475534\nAnchorURL              0.123709\nPrefixSuffix-          0.057779\nServerFormHandler      0.032643\nWebsiteTraffic         0.024551\nLinksInScriptTags      0.022267\nRequestURL             0.020588\nShortURL               0.019069\nSubDomains             0.018667\nDNSRecording           0.018476\nLinksPointingToPage    0.014928\nGoogleIndex            0.014698\nLongURL                0.014462\nStatusBarCust          0.014311\nRedirecting//          0.011992\nUsingIP                0.011905\nPageRank               0.010342\nHTTPSDomainURL         0.009686\nAgeofDomain            0.009104\nInfoEmail              0.008937\nIndex                  0.008531\nDomainRegLen           0.008518\nFavicon                0.008076\nUsingPopupWindow       0.007778\nWebsiteForwarding      0.007017\nAbnormalURL            0.006135\nIframeRedirection      0.005362\nNonStdPort             0.005272\nStatsReport            0.004022\nDisableRightClick      0.002853\nSymbol@                0.002788\ndtype: float32\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Training Neural Network model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scikeras.wrappers import KerasClassifier\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Define Neural Network\ndef create_nn():\n    model = Sequential([\n        Input(shape=(len(selected_features),)),\n        Dense(16, activation='relu'),\n        Dropout(0.3),\n        Dense(8, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Custom Keras wrapper with early stopping\nclass CalibratedKerasClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, epochs=50, batch_size=32):\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.model = None\n        self.calibrated = None\n    \n    def fit(self, X, y):\n        self.model = create_nn()\n        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, \n                       validation_split=0.2, callbacks=[early_stopping], verbose=0)\n        self.calibrated = CalibratedClassifierCV(\n            KerasClassifier(model=create_nn, epochs=self.epochs, batch_size=self.batch_size, verbose=0),\n            method='sigmoid', cv=5\n        )\n        self.calibrated.fit(X, y)\n        return self\n    \n    def predict(self, X):\n        return self.calibrated.predict(X)\n    \n    def predict_proba(self, X):\n        return self.calibrated.predict_proba(X)\n\n# Train Neural Network\nnn_model = CalibratedKerasClassifier(epochs=50, batch_size=32)\nnn_model.fit(X_train_scaled, y_train)\njoblib.dump(nn_model, '/kaggle/working/calibrated_neural_network_model.pkl')\nprint(\"Neural Network model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:45:57.417663Z","iopub.execute_input":"2025-06-17T05:45:57.417916Z","iopub.status.idle":"2025-06-17T05:49:24.954563Z","shell.execute_reply.started":"2025-06-17T05:45:57.417890Z","shell.execute_reply":"2025-06-17T05:49:24.953906Z"}},"outputs":[{"name":"stderr","text":"2025-06-17 05:45:58.777175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750139158.962031      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750139159.014356      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1750139171.333610      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1750139174.298526     132 service.cc:148] XLA service 0x7e960c00a9e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1750139174.299066     132 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1750139174.561174     132 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1750139175.861823     132 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Neural Network model trained and saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Testing Neural Network model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate Neural Network\ny_pred_nn = nn_model.predict(X_test_scaled)\ny_pred_proba_nn = nn_model.predict_proba(X_test_scaled)[:, 1]\nnn_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_nn),\n    'Precision': precision_score(y_test, y_pred_nn, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_nn, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_nn, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_nn)\n}\ncm_nn = confusion_matrix(y_test, y_pred_nn)\nprint(\"\\nNeural Network Metrics:\")\nprint(f\"Accuracy: {nn_results['Accuracy']:.4f}\")\nprint(f\"Precision: {nn_results['Precision']:.4f}\")\nprint(f\"Recall: {nn_results['Recall']:.4f}\")\nprint(f\"F1-Score: {nn_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {nn_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_nn}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('Neural Network Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/neural_network_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_nn, recall_nn, thresholds_nn = precision_recall_curve(y_test, y_pred_proba_nn)\noptimal_idx_nn = np.argmax(precision_nn * recall_nn)\noptimal_threshold_nn = thresholds_nn[optimal_idx_nn]\nprint(f\"Neural Network Optimal Threshold: {optimal_threshold_nn:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:51:32.660665Z","iopub.execute_input":"2025-06-17T05:51:32.661696Z","iopub.status.idle":"2025-06-17T05:51:35.079577Z","shell.execute_reply.started":"2025-06-17T05:51:32.661670Z","shell.execute_reply":"2025-06-17T05:51:35.078652Z"}},"outputs":[{"name":"stdout","text":"\nNeural Network Metrics:\nAccuracy: 0.9553\nPrecision: 0.9457\nRecall: 0.9645\nF1-Score: 0.9550\nROC-AUC: 0.9925\nConfusion Matrix:\n[[1185   67]\n [  43 1168]]\nNeural Network Optimal Threshold: 0.60\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Comparing models and conclusion\nimport pandas as pd\n\n# Combine results\nresults = {\n    'Random Forest': rf_results,\n    'XGBoost': xgb_results,\n    'Neural Network': nn_results\n}\nresults_df = pd.DataFrame(results).T\nprint(\"\\nModel Comparison:\\n\", results_df)\n\n# Identify best model\nbest_model_name = results_df['F1-Score'].idxmax()\nbest_model_results = results[best_model_name]\nprint(f\"\\nBest Model: {best_model_name} (F1-Score: {best_model_results['F1-Score']:.4f})\")\n\n# Save best model\nmodels = {'Random Forest': rf_model, 'XGBoost': xgb_model, 'Neural Network': nn_model}\nbest_model = models[best_model_name]\njoblib.dump(best_model, f'/kaggle/working/calibrated_{best_model_name.lower().replace(\" \", \"_\")}_model_v9.pkl')\nprint(f\"Saved calibrated_{best_model_name.lower().replace(' ', '_')}_model_v9.pkl to /kaggle/working/\")\n\n# Prediction function\ndef extract_features(url):\n    try:\n        parsed_url = urllib.parse.urlparse(url if url.startswith(('http://', 'https://')) else f'http://{url}')\n        extracted = tldextract.extract(url)\n        domain_len = len(extracted.domain)\n        subdomain_count = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n        path = parsed_url.path.lower()\n        domain = extracted.domain.lower()\n        suspicious_keywords = ['login', 'secure', 'account', 'verify', 'update', 'auth']\n        tld = extracted.suffix.lower()\n        is_local_ip = parsed_url.hostname and parsed_url.hostname.startswith('192.168.')\n        entropy = calculate_entropy(domain)\n        https = 1 if parsed_url.scheme == 'https' else -1\n        features = {\n            'AnchorURL': 1 if any(kw in path or kw in domain for kw in suspicious_keywords) else -1,\n            'WebsiteTraffic': 1 if domain_len < 7 and tld in ['com', 'org', 'edu', 'gov'] else -1,\n            'AgeofDomain': 1 if domain_len < 9 and tld in ['com', 'org', 'gov'] else -1,\n            'PageRank': 1 if domain_len < 7 and tld in ['com', 'org', 'edu', 'gov'] else -1,\n            'PrefixSuffix-': 1 if '-' in extracted.domain else -1,\n            'UsingIP': 1 if is_local_ip else -2 if parsed_url.hostname and parsed_url.hostname.replace('.', '').isdigit() else -1,\n            'SubDomains': -4 if subdomain_count >= 2 else -2 if subdomain_count == 1 else 0,\n            'Entropy': entropy,\n            'HTTPS': https\n        }\n        return features\n    except Exception as e:\n        print(f\"Error processing URL {url}: {e}\")\n        return {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, \n                'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 0, 'HTTPS': -1}\n\ndef predict_safety(url, model, scaler, threshold=0.5):\n    features = extract_features(url)\n    input_df = pd.DataFrame([features], columns=selected_features)\n    input_df = input_df.astype(float)\n    input_df_scaled = scaler.transform(input_df)\n    safety_score = model.predict_proba(input_df_scaled)[0, 0] * 100\n    label = 'Likely Safe' if safety_score >= (1 - threshold) * 100 else 'Likely Phishing'\n    return {'url': url, 'safety_score': round(safety_score, 2), 'label': label, 'features': features}\n\n# Test URLs\ntest_urls = [\n    \"https://www.google.com\",\n    \"https://fake-bank-login.com\",\n    \"http://192.168.1.1/login\",\n    \"https://paypal-security-login.com\",\n    \"https://www.wikipedia.org\",\n    \"https://www.netflix.com\",\n    \"https://secure-login-bank.com\",\n    \"https://www.linkedin.com\",\n    \"http://update-your-account.com\",\n    \"https://www.bbc.co.uk\",\n    \"https://gov.engdwpid.icu/cn\",\n    \"https://skymailre-validate.weebly.com/\"\n]\n\n# Test predictions\noptimal_threshold = {'Random Forest': optimal_threshold_rf, 'XGBoost': optimal_threshold_xgb, 'Neural Network': optimal_threshold_nn}\nprint(f\"\\n{best_model_name} URL Safety Predictions (Threshold: {optimal_threshold[best_model_name]:.2f}):\")\nfor url in test_urls:\n    result = predict_safety(url, best_model, scaler, optimal_threshold[best_model_name])\n    print(f\"URL: {result['url']} -> Safety Score: {result['safety_score']}% ({result['label']})\")\n    print(f\"Features: {result['features']}\")\n\n# Conclusion\nprint(\"\\nConclusion:\")\nprint(f\"The {best_model_name} model performed the best with an F1-Score of {best_model_results['F1-Score']:.4f}. \"\n      \"It demonstrates strong capability in distinguishing between legitimate and phishing URLs. \"\n      \"Key features influencing predictions include Entropy, HTTPS, and AnchorURL, as seen in feature importance (for tree-based models). \"\n      \"The model is saved and can be used for real-time URL safety predictions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:51:56.683196Z","iopub.execute_input":"2025-06-17T05:51:56.683475Z","iopub.status.idle":"2025-06-17T05:51:57.239686Z","shell.execute_reply.started":"2025-06-17T05:51:56.683455Z","shell.execute_reply":"2025-06-17T05:51:57.239092Z"}},"outputs":[{"name":"stdout","text":"\nModel Comparison:\n                 Accuracy  Precision    Recall  F1-Score   ROC-AUC\nRandom Forest   0.968331   0.960195  0.976053  0.968059  0.996825\nXGBoost         0.965895   0.960000  0.971098  0.965517  0.996506\nNeural Network  0.955339   0.945749  0.964492  0.955029  0.992461\n\nBest Model: Random Forest (F1-Score: 0.9681)\nSaved calibrated_random_forest_model_v9.pkl to /kaggle/working/\n\nRandom Forest URL Safety Predictions (Threshold: 0.64):\nURL: https://www.google.com -> Safety Score: 87.64% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': 1, 'AgeofDomain': 1, 'PageRank': 1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 1.9182958340544893, 'HTTPS': 1}\nURL: https://fake-bank-login.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.373557262275185, 'HTTPS': 1}\nURL: http://192.168.1.1/login -> Safety Score: 89.73% (Likely Safe)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': 1, 'SubDomains': 0, 'Entropy': 2.2998963911678914, 'HTTPS': -1}\nURL: https://paypal-security-login.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.820888851350188, 'HTTPS': 1}\nURL: https://www.wikipedia.org -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.6416041678685933, 'HTTPS': 1}\nURL: https://www.netflix.com -> Safety Score: 94.73% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': 1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.8073549220576046, 'HTTPS': 1}\nURL: https://secure-login-bank.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.734521664779752, 'HTTPS': 1}\nURL: https://www.linkedin.com -> Safety Score: 94.73% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': 1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.5, 'HTTPS': 1}\nURL: http://update-your-account.com -> Safety Score: 9.05% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.47135448701393, 'HTTPS': -1}\nURL: https://www.bbc.co.uk -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 0.9182958340544896, 'HTTPS': 1}\nURL: https://gov.engdwpid.icu/cn -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.75, 'HTTPS': 1}\nURL: https://skymailre-validate.weebly.com/ -> Safety Score: 87.64% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': 1, 'AgeofDomain': 1, 'PageRank': 1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.2516291673878226, 'HTTPS': 1}\n\nConclusion:\nThe Random Forest model performed the best with an F1-Score of 0.9681. It demonstrates strong capability in distinguishing between legitimate and phishing URLs. Key features influencing predictions include Entropy, HTTPS, and AnchorURL, as seen in feature importance (for tree-based models). The model is saved and can be used for real-time URL safety predictions.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Training LightGBM model\nfrom lightgbm import LGBMClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport joblib\n\n# Train LightGBM\nlgbm_model = CalibratedClassifierCV(\n    LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, verbose=-1),\n    method='sigmoid', cv=5\n)\nlgbm_model.fit(X_train_scaled, y_train)\njoblib.dump(lgbm_model, '/kaggle/working/calibrated_lightgbm_model.pkl')\nprint(\"LightGBM model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:57:06.255165Z","iopub.execute_input":"2025-06-17T05:57:06.255857Z","iopub.status.idle":"2025-06-17T05:57:11.484106Z","shell.execute_reply.started":"2025-06-17T05:57:06.255835Z","shell.execute_reply":"2025-06-17T05:57:11.480894Z"}},"outputs":[{"name":"stdout","text":"LightGBM model trained and saved.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Testing LightGBM model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate LightGBM\ny_pred_lgbm = lgbm_model.predict(X_test_scaled)\ny_pred_proba_lgbm = lgbm_model.predict_proba(X_test_scaled)[:, 1]\nlgbm_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_lgbm),\n    'Precision': precision_score(y_test, y_pred_lgbm, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_lgbm, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_lgbm, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_lgbm)\n}\ncm_lgbm = confusion_matrix(y_test, y_pred_lgbm)\nprint(\"\\nLightGBM Metrics:\")\nprint(f\"Accuracy: {lgbm_results['Accuracy']:.4f}\")\nprint(f\"Precision: {lgbm_results['Precision']:.4f}\")\nprint(f\"Recall: {lgbm_results['Recall']:.4f}\")\nprint(f\"F1-Score: {lgbm_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {lgbm_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_lgbm}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_lgbm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('LightGBM Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/lightgbm_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_lgbm, recall_lgbm, thresholds_lgbm = precision_recall_curve(y_test, y_pred_proba_lgbm)\noptimal_idx_lgbm = np.argmax(precision_lgbm * recall_lgbm)\noptimal_threshold_lgbm = thresholds_lgbm[optimal_idx_lgbm]\nprint(f\"LightGBM Optimal Threshold: {optimal_threshold_lgbm:.2f}\")\n\n# Feature importance\nfeature_importance_lgbm = pd.Series(lgbm_model.calibrated_classifiers_[0].estimator.feature_importances_, index=selected_features)\nprint(\"\\nLightGBM Feature Importances:\\n\", feature_importance_lgbm.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:57:36.254131Z","iopub.execute_input":"2025-06-17T05:57:36.255203Z","iopub.status.idle":"2025-06-17T05:57:36.445196Z","shell.execute_reply.started":"2025-06-17T05:57:36.255175Z","shell.execute_reply":"2025-06-17T05:57:36.444412Z"}},"outputs":[{"name":"stdout","text":"\nLightGBM Metrics:\nAccuracy: 0.9655\nPrecision: 0.9600\nRecall: 0.9703\nF1-Score: 0.9651\nROC-AUC: 0.9966\nConfusion Matrix:\n[[1203   49]\n [  36 1175]]\nLightGBM Optimal Threshold: 0.70\n\nLightGBM Feature Importances:\n Index                  480\nHTTPS                  257\nWebsiteTraffic         248\nLinksInScriptTags      246\nSubDomains             229\nAnchorURL              212\nLinksPointingToPage    148\nRequestURL             123\nAgeofDomain             95\nPrefixSuffix-           87\nServerFormHandler       78\nDomainRegLen            76\nPageRank                69\nDNSRecording            66\nIframeRedirection       59\nGoogleIndex             59\nUsingIP                 57\nLongURL                 51\nShortURL                48\nInfoEmail               46\nUsingPopupWindow        39\nFavicon                 35\nRedirecting//           32\nWebsiteForwarding       28\nAbnormalURL             28\nNonStdPort              24\nHTTPSDomainURL          22\nDisableRightClick       19\nStatusBarCust           15\nSymbol@                 13\nStatsReport             11\ndtype: int32\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Training CatBoost model\nfrom catboost import CatBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport joblib\n\n# Train CatBoost\ncatboost_model = CalibratedClassifierCV(\n    CatBoostClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, verbose=0),\n    method='sigmoid', cv=5\n)\ncatboost_model.fit(X_train_scaled, y_train)\njoblib.dump(catboost_model, '/kaggle/working/calibrated_catboost_model.pkl')\nprint(\"CatBoost model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:58:00.843138Z","iopub.execute_input":"2025-06-17T05:58:00.843843Z","iopub.status.idle":"2025-06-17T05:58:03.784429Z","shell.execute_reply.started":"2025-06-17T05:58:00.843814Z","shell.execute_reply":"2025-06-17T05:58:03.783662Z"}},"outputs":[{"name":"stdout","text":"CatBoost model trained and saved.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Testing CatBoost model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate CatBoost\ny_pred_catboost = catboost_model.predict(X_test_scaled)\ny_pred_proba_catboost = catboost_model.predict_proba(X_test_scaled)[:, 1]\ncatboost_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_catboost),\n    'Precision': precision_score(y_test, y_pred_catboost, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_catboost, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_catboost, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_catboost)\n}\ncm_catboost = confusion_matrix(y_test, y_pred_catboost)\nprint(\"\\nCatBoost Metrics:\")\nprint(f\"Accuracy: {catboost_results['Accuracy']:.4f}\")\nprint(f\"Precision: {catboost_results['Precision']:.4f}\")\nprint(f\"Recall: {catboost_results['Recall']:.4f}\")\nprint(f\"F1-Score: {catboost_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {catboost_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_catboost}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_catboost, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('CatBoost Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/catboost_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_catboost, recall_catboost, thresholds_catboost = precision_recall_curve(y_test, y_pred_proba_catboost)\noptimal_idx_catboost = np.argmax(precision_catboost * recall_catboost)\noptimal_threshold_catboost = thresholds_catboost[optimal_idx_catboost]\nprint(f\"CatBoost Optimal Threshold: {optimal_threshold_catboost:.2f}\")\n\n# Feature importance\nfeature_importance_catboost = pd.Series(catboost_model.calibrated_classifiers_[0].estimator.feature_importances_, index=selected_features)\nprint(\"\\nCatBoost Feature Importances:\\n\", feature_importance_catboost.sort_values(ascending=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:58:18.061567Z","iopub.execute_input":"2025-06-17T05:58:18.061889Z","iopub.status.idle":"2025-06-17T05:58:18.191275Z","shell.execute_reply.started":"2025-06-17T05:58:18.061868Z","shell.execute_reply":"2025-06-17T05:58:18.190395Z"}},"outputs":[{"name":"stdout","text":"\nCatBoost Metrics:\nAccuracy: 0.9675\nPrecision: 0.9624\nRecall: 0.9719\nF1-Score: 0.9671\nROC-AUC: 0.9964\nConfusion Matrix:\n[[1206   46]\n [  34 1177]]\nCatBoost Optimal Threshold: 0.56\n\nCatBoost Feature Importances:\n AnchorURL              20.821653\nHTTPS                  15.746782\nWebsiteTraffic          9.538633\nSubDomains              6.193686\nPrefixSuffix-           5.505230\nIndex                   4.666806\nServerFormHandler       4.342038\nLinksInScriptTags       4.092566\nLinksPointingToPage     3.698499\nDNSRecording            3.379207\nRequestURL              3.182867\nUsingIP                 3.058244\nAgeofDomain             2.671087\nPageRank                1.889003\nDomainRegLen            1.749532\nGoogleIndex             1.724020\nLongURL                 1.359689\nFavicon                 0.923789\nAbnormalURL             0.905219\nUsingPopupWindow        0.698898\nInfoEmail               0.665896\nHTTPSDomainURL          0.646896\nRedirecting//           0.603568\nIframeRedirection       0.510801\nWebsiteForwarding       0.385117\nShortURL                0.278762\nNonStdPort              0.256599\nStatusBarCust           0.216826\nStatsReport             0.139596\nDisableRightClick       0.079775\nSymbol@                 0.068719\ndtype: float64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Training SVM model\nfrom sklearn.svm import SVC\nfrom sklearn.calibration import CalibratedClassifierCV\nimport joblib\n\n# Train SVM\nsvm_model = CalibratedClassifierCV(\n    SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42),\n    method='sigmoid', cv=5\n)\nsvm_model.fit(X_train_scaled, y_train)\njoblib.dump(svm_model, '/kaggle/working/calibrated_svm_model.pkl')\nprint(\"SVM model trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:58:33.222642Z","iopub.execute_input":"2025-06-17T05:58:33.223362Z","iopub.status.idle":"2025-06-17T05:59:00.970152Z","shell.execute_reply.started":"2025-06-17T05:58:33.223343Z","shell.execute_reply":"2025-06-17T05:59:00.968691Z"}},"outputs":[{"name":"stdout","text":"SVM model trained and saved.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Testing SVM model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate SVM\ny_pred_svm = svm_model.predict(X_test_scaled)\ny_pred_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1]\nsvm_results = {\n    'Accuracy': accuracy_score(y_test, y_pred_svm),\n    'Precision': precision_score(y_test, y_pred_svm, pos_label=1),\n    'Recall': recall_score(y_test, y_pred_svm, pos_label=1),\n    'F1-Score': f1_score(y_test, y_pred_svm, pos_label=1),\n    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_svm)\n}\ncm_svm = confusion_matrix(y_test, y_pred_svm)\nprint(\"\\nSVM Metrics:\")\nprint(f\"Accuracy: {svm_results['Accuracy']:.4f}\")\nprint(f\"Precision: {svm_results['Precision']:.4f}\")\nprint(f\"Recall: {svm_results['Recall']:.4f}\")\nprint(f\"F1-Score: {svm_results['F1-Score']:.4f}\")\nprint(f\"ROC-AUC: {svm_results['ROC-AUC']:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm_svm}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Legitimate', 'Phishing'], yticklabels=['Legitimate', 'Phishing'])\nplt.title('SVM Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.savefig('/kaggle/working/svm_cm.png')\nplt.close()\n\n# Find optimal threshold\nprecision_svm, recall_svm, thresholds_svm = precision_recall_curve(y_test, y_pred_proba_svm)\noptimal_idx_svm = np.argmax(precision_svm * recall_svm)\noptimal_threshold_svm = thresholds_svm[optimal_idx_svm]\nprint(f\"SVM Optimal Threshold: {optimal_threshold_svm:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T05:59:32.918235Z","iopub.execute_input":"2025-06-17T05:59:32.918542Z","iopub.status.idle":"2025-06-17T05:59:35.988086Z","shell.execute_reply.started":"2025-06-17T05:59:32.918518Z","shell.execute_reply":"2025-06-17T05:59:35.987085Z"}},"outputs":[{"name":"stdout","text":"\nSVM Metrics:\nAccuracy: 0.9566\nPrecision: 0.9473\nRecall: 0.9653\nF1-Score: 0.9562\nROC-AUC: 0.9899\nConfusion Matrix:\n[[1187   65]\n [  42 1169]]\nSVM Optimal Threshold: 0.59\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Preprocessing for LSTM model\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n# Check for URL column\nif 'URL' in csv_data.columns:\n    # Prepare URL character sequences\n    max_len = 100  # Maximum URL length\n    tokenizer = Tokenizer(char_level=True, lower=True)\n    tokenizer.fit_on_texts(csv_data['URL'])\n    X_seq = tokenizer.texts_to_sequences(csv_data['URL'])\n    X_seq_padded = pad_sequences(X_seq, maxlen=max_len, padding='post', truncating='post')\n\n    # Split data for LSTM\n    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq_padded, y_aug, test_size=0.2, random_state=42)\n    print(\"LSTM Training Shape:\", X_train_seq.shape, y_train_seq.shape)\n    print(\"LSTM Testing Shape:\", X_test_seq.shape, y_test_seq.shape)\nelse:\n    print(\"No 'URL' column found in dataset. Skipping LSTM preprocessing.\")\n    X_train_seq, X_test_seq, y_train_seq, y_test_seq = None, None, None, None\n    tokenizer = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T07:22:14.412349Z","iopub.execute_input":"2025-06-17T07:22:14.412660Z","iopub.status.idle":"2025-06-17T07:22:14.419627Z","shell.execute_reply.started":"2025-06-17T07:22:14.412638Z","shell.execute_reply":"2025-06-17T07:22:14.418802Z"}},"outputs":[{"name":"stdout","text":"No 'URL' column found in dataset. Skipping LSTM preprocessing.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Comparing all models and conclusion\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport joblib\n\n# Combine results (exclude LSTM if not preprocessed)\nresults = {\n    'Random Forest': rf_results,\n    'XGBoost': xgb_results,\n    'Neural Network': nn_results,\n    'LightGBM': lgbm_results,\n    'CatBoost': catboost_results,\n    'SVM': svm_results\n}\nif 'URL' in csv_data.columns and 'lstm_results' in globals():\n    results['LSTM'] = lstm_results\n\nresults_df = pd.DataFrame(results).T\nprint(\"\\nModel Comparison:\\n\", results_df)\n\n# Identify best model\nbest_model_name = results_df['F1-Score'].idxmax()\nbest_model_results = results[best_model_name]\nprint(f\"\\nBest Model: {best_model_name} (F1-Score: {best_model_results['F1-Score']:.4f})\")\n\n# Plot ROC curves\nplt.figure(figsize=(8, 6))\nmodels = {\n    'Random Forest': rf_model,\n    'XGBoost': xgb_model,\n    'Neural Network': nn_model,\n    'LightGBM': lgbm_model,\n    'CatBoost': catboost_model,\n    'SVM': svm_model\n}\nfor name, model in models.items():\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_pred_proba):.2f})')\nif 'URL' in csv_data.columns and 'lstm_model' in globals():\n    y_pred_proba_lstm = lstm_model.predict_proba(X_test_seq)[:, 1]\n    fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_proba_lstm)\n    plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_score(y_test_seq, y_pred_proba_lstm):.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves for All Models')\nplt.legend()\nplt.savefig('/kaggle/working/roc_curves_all_models.png')\nplt.close()\n\n# Save best model\nmodels_all = {\n    'Random Forest': rf_model,\n    'XGBoost': xgb_model,\n    'Neural Network': nn_model,\n    'LightGBM': lgbm_model,\n    'CatBoost': catboost_model,\n    'SVM': svm_model\n}\nif 'URL' in csv_data.columns and 'lstm_model' in globals():\n    models_all['LSTM'] = lstm_model\nbest_model = models_all[best_model_name]\njoblib.dump(best_model, f'/kaggle/working/calibrated_{best_model_name.lower().replace(\" \", \"_\")}_model_v10.pkl')\nprint(f\"Saved calibrated_{best_model_name.lower().replace(' ', '_')}_model_v10.pkl to /kaggle/working/\")\n\n# Update predict_safety for LSTM compatibility\ndef predict_safety(url, model, scaler, tokenizer=None, max_len=100, threshold=0.5, is_lstm=False):\n    if is_lstm and tokenizer is not None:\n        seq = tokenizer.texts_to_sequences([url])\n        seq_padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n        safety_score = model.predict_proba(seq_padded)[0, 0] * 100\n        label = 'Likely Safe' if safety_score >= (1 - threshold) * 100 else 'Likely Phishing'\n        return {'url': url, 'safety_score': round(safety_score, 2), 'label': label, 'features': 'Character sequences'}\n    else:\n        features = extract_features(url)\n        input_df = pd.DataFrame([features], columns=selected_features)\n        input_df = input_df.astype(float)\n        input_df_scaled = scaler.transform(input_df)\n        safety_score = model.predict_proba(input_df_scaled)[0, 0] * 100\n        label = 'Likely Safe' if safety_score >= (1 - threshold) * 100 else 'Likely Phishing'\n        return {'url': url, 'safety_score': round(safety_score, 2), 'label': label, 'features': features}\n\n# Test URLs\ntest_urls = [\n    \"https://www.google.com\",\n    \"https://fake-bank-login.com\",\n    \"http://192.168.1.1/login\",\n    \"https://paypal-security-login.com\",\n    \"https://www.wikipedia.org\",\n    \"https://www.netflix.com\",\n    \"https://secure-login-bank.com\",\n    \"https://www.linkedin.com\",\n    \"http://update-your-account.com\",\n    \"https://www.bbc.co.uk\",\n    \"https://gov.engdwpid.icu/cn\",\n    \"https://skymailre-validate.weebly.com/\"\n]\n\n# Test predictions with best model\noptimal_thresholds = {\n    'Random Forest': optimal_threshold_rf,\n    'XGBoost': optimal_threshold_xgb,\n    'Neural Network': optimal_threshold_nn,\n    'LightGBM': optimal_threshold_lgbm,\n    'CatBoost': optimal_threshold_catboost,\n    'SVM': optimal_threshold_svm\n}\nif 'URL' in csv_data.columns and 'lstm_model' in globals():\n    optimal_thresholds['LSTM'] = optimal_threshold_lstm\n\nprint(f\"\\n{best_model_name} URL Safety Predictions (Threshold: {optimal_thresholds[best_model_name]:.2f}):\")\nfor url in test_urls:\n    result = predict_safety(url, best_model, scaler, tokenizer, max_len, optimal_thresholds[best_model_name], is_lstm=(best_model_name == 'LSTM'))\n    print(f\"URL: {result['url']} -> Safety Score: {result['safety_score']}% ({result['label']})\")\n    print(f\"Features: {result['features']}\")\n\n# Conclusion\nprint(\"\\nConclusion:\")\nprint(f\"The {best_model_name} model achieved the highest F1-Score of {best_model_results['F1-Score']:.4f}. \"\n      \"Among the new models, LightGBM, CatBoost, and SVM provide strong alternatives. \"\n      \"The LSTM model was skipped or included based on the availability of raw URLs. \"\n      \"Tree-based models (Random Forest, LightGBM, CatBoost) often excel due to feature importance insights, while SVM offers robust performance for non-linear patterns. \"\n      \"The best model is saved for deployment.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T07:24:39.284107Z","iopub.execute_input":"2025-06-17T07:24:39.284424Z","iopub.status.idle":"2025-06-17T07:24:43.003373Z","shell.execute_reply.started":"2025-06-17T07:24:39.284399Z","shell.execute_reply":"2025-06-17T07:24:43.002584Z"}},"outputs":[{"name":"stdout","text":"\nModel Comparison:\n                 Accuracy  Precision    Recall  F1-Score   ROC-AUC\nRandom Forest   0.968331   0.960195  0.976053  0.968059  0.996825\nXGBoost         0.965895   0.960000  0.971098  0.965517  0.996506\nNeural Network  0.955339   0.945749  0.964492  0.955029  0.992461\nLightGBM        0.965489   0.959967  0.970273  0.965092  0.996634\nCatBoost        0.967519   0.962388  0.971924  0.967132  0.996417\nSVM             0.956557   0.947326  0.965318  0.956237  0.989886\n\nBest Model: Random Forest (F1-Score: 0.9681)\nSaved calibrated_random_forest_model_v10.pkl to /kaggle/working/\n\nRandom Forest URL Safety Predictions (Threshold: 0.64):\nURL: https://www.google.com -> Safety Score: 87.64% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': 1, 'AgeofDomain': 1, 'PageRank': 1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 1.9182958340544893, 'HTTPS': 1}\nURL: https://fake-bank-login.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.373557262275185, 'HTTPS': 1}\nURL: http://192.168.1.1/login -> Safety Score: 89.73% (Likely Safe)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': 1, 'SubDomains': 0, 'Entropy': 2.2998963911678914, 'HTTPS': -1}\nURL: https://paypal-security-login.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.820888851350188, 'HTTPS': 1}\nURL: https://www.wikipedia.org -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.6416041678685933, 'HTTPS': 1}\nURL: https://www.netflix.com -> Safety Score: 94.73% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': 1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.8073549220576046, 'HTTPS': 1}\nURL: https://secure-login-bank.com -> Safety Score: 1.4% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.734521664779752, 'HTTPS': 1}\nURL: https://www.linkedin.com -> Safety Score: 94.73% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': 1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.5, 'HTTPS': 1}\nURL: http://update-your-account.com -> Safety Score: 9.05% (Likely Phishing)\nFeatures: {'AnchorURL': 1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': 1, 'UsingIP': -1, 'SubDomains': 0, 'Entropy': 3.47135448701393, 'HTTPS': -1}\nURL: https://www.bbc.co.uk -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 0.9182958340544896, 'HTTPS': 1}\nURL: https://gov.engdwpid.icu/cn -> Safety Score: 95.56% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': -1, 'AgeofDomain': -1, 'PageRank': -1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.75, 'HTTPS': 1}\nURL: https://skymailre-validate.weebly.com/ -> Safety Score: 87.64% (Likely Safe)\nFeatures: {'AnchorURL': -1, 'WebsiteTraffic': 1, 'AgeofDomain': 1, 'PageRank': 1, 'PrefixSuffix-': -1, 'UsingIP': -1, 'SubDomains': -2, 'Entropy': 2.2516291673878226, 'HTTPS': 1}\n\nConclusion:\nThe Random Forest model achieved the highest F1-Score of 0.9681. Among the new models, LightGBM, CatBoost, and SVM provide strong alternatives. The LSTM model was skipped or included based on the availability of raw URLs. Tree-based models (Random Forest, LightGBM, CatBoost) often excel due to feature importance insights, while SVM offers robust performance for non-linear patterns. The best model is saved for deployment.\n","output_type":"stream"}],"execution_count":33}]}